{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in Deep Neural Networks\n",
    "\n",
    "\n",
    "#TODO: unceratity scaling\n",
    "# TODO thompson gridsearch NLPD\n",
    "# TODO Thompson gridsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#%latex\n",
    "# Introduction\n",
    "\n",
    "## Uncertainty\n",
    "### What is uncertainty\n",
    "### Why do we need uncertainty\n",
    "### What does uncertainty express\n",
    "### Can we quantify if it's good PU or is that impossible\n",
    "By definition, uncertainty is a number quantifying unknowns about a thing. It is however possible to say something is 'good' unknowing vs 'bad' unknowing: \n",
    "Good unknowing would actually contain unknowns, while 'bad' unknowing would also ignore knowledge that we have. \n",
    "\n",
    "Maximum entropy principle should apply to Posterior the same way it applies to the Prior. In simple words, this means that the posterior should be as informative as possible while not making up additional information\n",
    "### Bayesian vs Frequentist\n",
    "\n",
    "\n",
    "### predictive Uncertainty: uncertainty about the mean vs estimation of error\n",
    "\n",
    "\n",
    "## Deep Neural Networks\n",
    "### What is Deep Neural Networks\n",
    "### why do we use Deep Neural Networks\n",
    "### Why do Deep Neural Networks not contain uncertainty\n",
    "\n",
    "\n",
    "## Approaches\n",
    "There are three distinct approaches that can extract predictive uncertainty\n",
    "\n",
    "### Distribution over Parameters (fully bayesian)\n",
    "#### Awesome, but intractable\n",
    "\n",
    "### Ensembles\n",
    "\n",
    "\n",
    "### one-network-estimators\n",
    "\n",
    "### Adversarial Training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Aims of this work\n",
    "This work aims to contribute threefold:\n",
    "\n",
    "1. by introducing a novel measurement for the quality of uncertainty, the 'Correlation Between Error and Uncertainty' (CoBEAU).\n",
    "2. by providing a comprehensive comparisson of the performance of several algorithms that can be used to transform the point estimate of a (Deep) Neural Network into a predictive (normal) distribution.\n",
    "3. by introducing several variations of the 'Bobstrap', an ensemble that is comprised of differently-aged versions of the model that are combined at prediction time to estimate predicitive uncertainty.\n",
    "\n",
    "The Appendix contains thoughts about how to extend the online Bootstrap towards models that rely on different epochs such as Neural Networks as well as a short analysis of some of the models in a online-bootstrap situation.\n",
    "## structure of this work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Methods\n",
    "## Datasets\n",
    "For self generated toy datasets, in each iteration of testing a new seed was passed, to test for similar but different datasets.\n",
    "\n",
    "The real life datasets have not been modified (apart from random shuffling) in each testing period\n",
    "### linear toy\n",
    "### nonlinear toy\n",
    "### the X^3 thing from the paper\n",
    "### Boston housing\n",
    "### Stock price\n",
    "\n",
    "## Hyperparameter optimisation\n",
    "Hyperparameters were determined in non-exhaustive grid search for each Dataset. Root mean squared error (RSME) was chosen as an optimisation criterion for two reasons: \n",
    "First, it is one of the the standards in regression tasks and as such well understood, easily implemented  and computationally efficient to obtain.\n",
    "Second, and more important, RSME is a measure that does not depend on predictive uncertainty but only on the predictive mean (which in many cases is just a simple prediction). This orthogonality allows for optimisation of model parameters while explicitly ignoring the behaviour of the predicitve uncertainty and thus avoiding explicit or implicit bias\\footnote{note however, that NLPD depends on the squared error. However, we are mainly interested in this measure's statement about PU, and thus optimising for this part of the equation will not notably influence it's expressivenes.}.\n",
    "\n",
    "See the appendix/tools for a more in depth description.\n",
    "## approaches\n",
    "### ensembles\n",
    "#### Ensemble Through Time\n",
    "##### What is being ensembled\n",
    "##### How to obtain uncertainty\n",
    "##### What the uncertainty stands for\n",
    "Change in belief. Since we update with the same number of epochs, this corresponds to bayesian update where the prior decays exponentially\n",
    "##### Does obtaining uncertainty influence the predictive power?\n",
    "#### Bootstrap\n",
    "#### Dimensionality Network\n",
    "#### Forced Diversity\n",
    "\n",
    "### one network approaches\n",
    "#### Dropout MCMC\n",
    "#### GaussianLoss\n",
    "This one is tricky: As [] hae shown, we can theoretically ise gradient descent to optimise any function. However, as we have seen [mention that], regression networks generally output only one value - the regression prediction. NLPD (and similar expressions) however contain two parameters - the mean and the variance; thus it is not immediately obvious, how we can use NLPD as a cost function. [] used a neat trick to circuvent this limitation: they essentialy built a two-headed Neural Network.\n",
    "\n",
    "#### LearningRateNetwork\n",
    "Can easily be implemented with GaussianLoss; technically this is implicitly done with dropout (since each dropout is technically a draw from the distribution that is modelled - so it is in average scaled by the predictice uncertainty)\n",
    "\n",
    "## Measures\n",
    "### NLPD\n",
    "Compare to 'optimal' NLPD - with no error!\n",
    "### CoBEAU\n",
    "Cobeau only describes the standard deviation relative to the error. NLPD adds a measure of fit - cobeau does not!\n",
    "### Coverage\n",
    "Kind of mostly depends on pu scaling\n",
    "### out of sample behaviour\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "# Disussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
